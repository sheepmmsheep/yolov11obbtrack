import streamlit as st
import cv2
from PIL import Image
import torch

from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import numpy as np
device = 'cpu'
if not hasattr(st, 'obb'):
    st.model = torch.hub.load('ultralytics/yolov11', 'custom', path="best.pt", _verbose=False)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)
st.title("YOLO å®æ—¶è¿½è¸ª with YOLO.track")
st.sidebar.title("æ“ä½œé€‰é¡¹")

conf_threshold = st.sidebar.slider("è¨­ç½®ä¿¡å¿ƒé–¥å€¼", min_value=0.0, max_value=1.0, value=0.35, step=0.05)
iou_threshold = st.sidebar.slider("è¨­ç½®IOUé–¥å€¼", min_value=0.0, max_value=1.0, value=0.3, step=0.05)

def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:
    img = frame.to_ndarray(format="bgr24")  

    results = st.model.track(source=img, conf=conf_threshold, iou=iou_threshold, persist=True)

    annotated_img = results[0].plot()

    return av.VideoFrame.from_ndarray(annotated_img, format="bgr24")

# å¯åŠ¨ WebRTC
webrtc_ctx = webrtc_streamer(
    key="yolo-tracking",  # å”¯ä¸€æ ‡è¯†ç¬¦
    mode=WebRtcMode.SENDRECV,  # åŒå‘æ¨¡å¼ï¼ˆæ¥æ”¶è§†é¢‘å¹¶å¤„ç†åè¿”å›ï¼‰
    video_frame_callback=video_frame_callback,  # è§†é¢‘å¤„ç†å›è°ƒ
    media_stream_constraints={"video": True, "audio": False},  # å¯ç”¨è§†é¢‘è¾“å…¥ï¼Œç¦ç”¨éŸ³é¢‘
    async_processing=True,  # å¯ç”¨å¼‚æ­¥å¤„ç†
)

# æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
if webrtc_ctx.state.playing:
    st.markdown("### æ­£åœ¨å®æ—¶è¿½è¸ªä¸­... ğŸ”")
    st.markdown("**è¯·ç¡®ä¿æ‘„åƒå¤´å·²å¯ç”¨å¹¶æ­£å¯¹åœºæ™¯ã€‚**")
else:
    st.markdown("### ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¯åŠ¨å®æ—¶è¿½è¸ªï¼")
